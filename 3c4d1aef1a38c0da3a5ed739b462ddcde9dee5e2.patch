diff --git a/fairseq/criterions/fairseq_criterion.py b/fairseq/criterions/fairseq_criterion.py
index d6a6823a..60a11f27 100644
--- a/fairseq/criterions/fairseq_criterion.py
+++ b/fairseq/criterions/fairseq_criterion.py
@@ -217,7 +217,7 @@ class MoECriterion(FairseqCriterion):
                 if isinstance(module, MOELayer):
                     total_val += module.metadata[key] if key in module.metadata else 0
                     count += 1
-            moe_logging_output[key] = total_val / count
+            moe_logging_output[key] = total_val / count if count > 0 else -1
         moe_logging_output["batch_count"] = 1
         return moe_logging_output
 
diff --git a/fairseq/modules/moe/moe_layer.py b/fairseq/modules/moe/moe_layer.py
index 1c1668d3..0bc2d345 100644
--- a/fairseq/modules/moe/moe_layer.py
+++ b/fairseq/modules/moe/moe_layer.py
@@ -195,7 +195,8 @@ class MOELayer(Base):
         self.in_generation = True
 
     def all_to_all_wrapper(self, input: Tensor):
-        dummy_a2a = getattr(self.args, 'dummy_a2a', False)
+        import os
+        dummy_a2a = (int(os.environ.get('SKIP_A2A', '0')) != 0)
         if dummy_a2a:
             input = input.contiguous()
             output = input.detach().clone()
diff --git a/fairseq/modules/transformer_layer.py b/fairseq/modules/transformer_layer.py
index 2d56ea0a..86ab2a88 100644
--- a/fairseq/modules/transformer_layer.py
+++ b/fairseq/modules/transformer_layer.py
@@ -367,6 +367,53 @@ class TransformerDecoderLayer(nn.Module):
                 self.quant_noise,
                 self.quant_noise_block_size,
             )
+        elif torch.os.environ.get('TUTEL') == '2': # Recommend if local_experts = 1 or you have custom expert list
+            print('Using Tutel = 2')
+            experts = make_experts(args, self.embed_dim, ffn_dim, self.dropout_module)
+
+            from tutel_moe.moe_layer import MOELayer as TutelMoeLayer
+            self.moe_layer = TutelMoeLayer(
+                'Top1Gate' if args.moe_top1_expert else 'Top2Gate',
+                model_dim = self.embed_dim,
+                external_experts = experts,
+                fp32_gate = args.moe_gating_use_fp32,
+                scan_experts = lambda name, param: setattr(param, 'expert', True),
+                result_func = lambda result: (result, result.l_aux),
+                group = dist_utils.get_data_parallel_group(),
+            )
+        elif torch.os.environ.get('TUTEL') == '1': # Recommend on any cases (for advanced developers)
+            print('Using Tutel = 1')
+            activation_fn = utils.get_activation_fn(activation=str(args.activation_fn) if getattr(args, "activation_fn", None) is not None else "relu")
+            activation_dropout_p = getattr(args, "activation_dropout", 0) or 0
+            if activation_dropout_p == 0:
+                activation_dropout_p = getattr(args, "relu_dropout", 0) or 0
+            activation_dropout_module = FairseqDropout(float(activation_dropout_p))
+            dropout_module = self.dropout_module
+            local_moe_expert_count = args.moe_expert_count // (1 if not torch.distributed.is_initialized() else torch.distributed.get_world_size())
+
+            from tutel_moe.moe_layer import MOELayer as TutelMoeLayer
+            def fused_custom_fn(m, x):
+                x_shape, x = x.shape, x.reshape(-1, x.size(-1))
+                if has_fused_bias_gelu and activation_fn == gelu:
+                  x = _linear(x, m.fc1_weight.view(-1, x.size(-1)))
+                  x = activation_dropout_module(fused_bias_gelu(x, m.fc1_bias))
+                  x = dropout_module(_linear(x, m.fc2_weight.view(-1, x.size(-1)), m.fc2_bias).view(x_shape))
+                else:
+                  x = _linear(x, m.fc1_weight.view(-1, x.size(-1)), m.fc1_bias)
+                  x = activation_fn(x)
+                  x = activation_dropout_module(x)
+                  x = dropout_module(_linear(x, m.fc2_weight.view(-1, x.size(-1)), m.fc2_bias).view(x_shape))
+                return x
+
+            self.moe_layer = TutelMoeLayer(
+                'Top1Gate' if args.moe_top1_expert else 'Top2Gate',
+                model_dim = self.embed_dim,
+                builtin_experts = {'type': 'ffn', 'count_per_node': local_moe_expert_count, 'hidden_size_per_expert': ffn_dim, 'fused_custom_fn': fused_custom_fn},
+                fp32_gate = args.moe_gating_use_fp32,
+                scan_experts = lambda name, param: setattr(param, 'expert', True),
+                result_func = lambda result: (result, result.l_aux),
+                group = dist_utils.get_data_parallel_group(),
+            )
         else:
 
             if args.moe_top1_expert:
diff --git a/fairseq/trainer.py b/fairseq/trainer.py
index f42d3140..bcab7538 100644
--- a/fairseq/trainer.py
+++ b/fairseq/trainer.py
@@ -733,7 +733,7 @@ class Trainer(object):
         self.criterion.train()
         self.zero_grad()
 
-        metrics.log_start_time("train_wall", priority=800, round=0)
+        metrics.log_start_time("train_wall", priority=800, round=3)
 
         # forward and backward pass
         logging_outputs, sample_size, ooms = [], 0, 0
